# -*- coding: utf-8 -*-
"""Web_Scraping_using_python_and_beautifulsoup.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X9w322c6qDRgwtnH7VEYiwxPFajObgQb

# **Web scraping using beautiful soup**

This notebook includes data scraping, which takes a website URL as an input and extracts the information listed below as an output from that webpage.


1.   Specific HTML tags along with titles and meta description
2.   Extract specific tags, heading tags from h1-h6 along with titles and meta description
3. Extracting ALT tags
4. For counting words inside a web page
5. Inspection of broken links inside a webpage
6. Extracting the source code of the webpage in google colab
7. Extracting all URLs from a website without duplication
8. Measuring the forntend and backend performance of website
"""

!pip install beautifulsoup4

"""**1. For scraping specific HTML tags along with titles and meta description**"""

#Importing libraries
from bs4 import BeautifulSoup
import urllib
from urllib import request
import urllib.request as ur

# Getting input for webiste from user
urlinput = input("Enter url :")
print(" This is the website link that you entered", urlinput)

# For extracting specific tags from webpage
def getTags(tag):
  s = ur.urlopen(urlinput)
  soup = BeautifulSoup(s.read())
  return soup.findAll(tag)

# For extracting specific title & meta description from webpage
def titleandmetaTags():
    s = ur.urlopen(urlinput)
    soup = BeautifulSoup(s.read())
    #----- Extracting Title from website ------#
    title = soup.title.string
    print ('Website Title is :', title)
    #-----  Extracting Meta description from website ------#
    meta_description = soup.find_all('meta')
    for tag in meta_description:
        if 'name' in tag.attrs.keys() and tag.attrs['name'].strip().lower() in ['description', 'keywords']:
            #print ('NAME    :',tag.attrs['name'].lower())
            print ('CONTENT :',tag.attrs['content'])

#------------- Main ---------------#
if __name__ == '__main__':
  titleandmetaTags()
  tags = getTags('h1')
  for tag in tags:
     print(tag) # display tags
     print(tag.contents) # display contents of the tags

"""**2. For extracting specific tags, all heading tags from h1-h6 along with titles and meta description**"""

# Importing libraries
from bs4 import BeautifulSoup
import urllib
from urllib import request
import urllib.request as ur

# Getting input for webiste from user
url_input = input("Enter url :")
print(" This is the website link that you entered", url_input)


# For extracting specific tags from webpage
def getTags(tag):
  s = ur.urlopen(url_input)
  soup = BeautifulSoup(s.read())
  return soup.findAll(tag)

# For extracting all h1-h6 heading tags from webpage
def headingTags(headingtags):
  h = ur.urlopen(url_input)
  soup = BeautifulSoup(h.read())
  print("List of headings from headingtags function h1, h2, h3, h4, h5, h6 :")
  for heading in soup.find_all(["h1", "h2", "h3", "h4", "h5", "h6"]):
    print(heading.name + ' ' + heading.text.strip())

# For extracting specific title & meta description from webpage
def titleandmetaTags():
    s = ur.urlopen(urlinput)
    soup = BeautifulSoup(s.read())
    #----- Extracting Title from website ------#
    title = soup.title.string
    print ('Website Title is :', title)
    #-----  Extracting Meta description from website ------#
    meta_description = soup.find_all('meta')
    for tag in meta_description:
        if 'name' in tag.attrs.keys() and tag.attrs['name'].strip().lower() in ['description', 'keywords']:
            #print ('NAME    :',tag.attrs['name'].lower())
            print ('CONTENT :',tag.attrs['content'])



#------------- Main ---------------#
if __name__ == '__main__':
  titleandmetaTags()
  tags = getTags('p')
  headtags = headingTags('h1')
  for tag in tags:
     print(" Here are the tags from getTags function:", tag.contents)

"""**3. For extracting ALT tags (Image Alter tags)**"""

import urllib.request as ur
from bs4 import BeautifulSoup

url_input = input("Enter url :")
print("The website link that you entered is:", url_input)

def alt_tag():
  url =  ur.urlopen(url_input)
  htmlSource = url.read()
  url.close()
  soup = BeautifulSoup(htmlSource)
  print('\n The alt tag along with the text in the web page')
  return soup


#------------- Main ---------------#
if __name__ == '__main__':
  soup_obj = alt_tag()
  print(soup_obj.find_all('img',alt= True))

"""**4. For counting words inside a web page**"""

import requests
from bs4 import BeautifulSoup
from collections import Counter
from string import punctuation

# Getting content from web page
r = requests.get("https://ulab.edu.bd/")
soup = BeautifulSoup(r.content)

# For getting words within paragrphs
text_paragraph = (''.join(s.findAll(text=True))for s in soup.findAll('p'))
count_paragraph = Counter((x.rstrip(punctuation).lower() for y in text_paragraph for x in y.split()))

# For getting words inside div tags
text_div = (''.join(s.findAll(text=True))for s in soup.findAll('div'))
count_div = Counter((x.rstrip(punctuation).lower() for y in text_div for x in y.split()))

# Adding two counters for getting a list with words count (from most to less common)
total = count_div + count_paragraph
list_most_common_words = total.most_common()

# Total words inside a webpage
len(total)

# List of common words
list_most_common_words

"""**5. For inspecting Broken links inside a webpage**

We want to retrieve the response code 200 if the site is fully functional. We'll get the 404 response code if it's not available.
"""

# Importing libraries
from bs4 import BeautifulSoup, SoupStrainer
import requests

# Getting URL from user
url = input("Enter your url: ")

def broken_page():
  # For making request to get the URL
  user_req_page = requests.get(url)

  # For getting the response code of given URL
  response_code = str(user_req_page.status_code)

  # For displaying the text of the URL in str
  data =user_req_page.text

  # For using BeautifulSoup to access the built-in methods
  soup = BeautifulSoup(data)

  # Iterate over all links on the given URL with the response code next to it i.e 404 for PAGE NOT FOUND, 200 if website is functional/available
  for link in soup.find_all('a'):
    print(f"Url: {link.get('href')} " + f"| Status Code: {response_code}")


#----- NOTE ------#
# --------- TO VERIFY PAGE NOT FOUND 404 ERROR, enter below web link as a input URL --------#
#https://roine.github.com/p1

#------------- Main ---------------#
if __name__ == '__main__':
  broken_page()

"""**6. For getting the source code of the webpage**

Here, we will be using 'page_source' method is used retrieve the page source of the webpage the user is currently accessing.

*NOTE: (Page source : The source code/page source is the programming behind any webpage)*
"""

# install chromium, its driver, and selenium
!apt update
!apt install chromium-chromedriver
!pip install selenium

# set options to be headless, ..
from selenium import webdriver
options = webdriver.ChromeOptions()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')

#------------- FOR DISPLAYING SOURCE CODE OF THE WEBPAGE -------------#

# open it, go to a website, and get results
wd = webdriver.Chrome(options=options)

# Prompt user to enter the URL
url = input("Enter your url: ")

# For making request to get the URL
wd.get(url)

# To display code results
print(wd.page_source)

"""**7. Extraction of all URLs from a website without duplication**"""

#---- Importing libraries ----#
import re
import requests
from bs4 import BeautifulSoup

all_links = set() #------ Creating a unique set of links ------#

for i in range(7):
   r = requests.get(("https://ulab.edu.bd/?page={}").format(i))
   soup = BeautifulSoup(r.content , "html.parser")
   for link in soup.find_all("a",href=re.compile('/')):
            link = (link.get('href'))
            #----- For the removal of duplicate URLs, We will simply add a link to that set; this assures that it's distinct ------#
            if link not in all_links:
              print(link)
            all_links.add(link)

"""**8. Measuring the forntend and backend performance of website**"""

#----- Installation of selenium and chromedriver in google colab -----#
!pip install selenium
!apt-get update
!apt install chromium-chromedriver

#---- Importing libraries ----#
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
import time
import csv
import os.path

#---- Accessing chromedriver in google colab ----#
from selenium import webdriver
options = webdriver.ChromeOptions()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')
wd = webdriver.Chrome(options=options)
driver =webdriver.Chrome(options=options)

#----- Creating csv file to write the calculated performance of the website
csv_path = "performance.csv"
file = open(csv_path, 'w', newline='')
writer = csv.writer(file)
writer.writerow(["backendPerformance_calc","frontendPerformance_calc"])


#----- Getting input for webiste from user
url = input("Enter url :")
print("This is the website link that you entered:", url)

#----- Setting iterations for testing the perfromance
iterations = 10
for i in range(iterations):
    driver =webdriver.Chrome(options=options)
    driver.get(url) #-- Passing url as parameter in Selenium method (driver.get)

    #-- Using Navigation Timing API to calculate the timings, Here driver.execute_script is called and the return value is stored in navigationStart
    #driver.execute_script then synchronously executes JavaScript in the current window or frame. In this case the ‘return window.performance.timing.navigationStart’ code will run.
    navigationStart = driver.execute_script("return window.performance.timing.navigationStart")
    responseStart = driver.execute_script("return window.performance.timing.responseStart")
    domComplete = driver.execute_script("return window.performance.timing.domComplete")

    backendPerformance_calc = responseStart - navigationStart
    frontendPerformance_calc = domComplete - responseStart

    #--This will print iteration wise backend and front end performance for website
    print("Iteration no:", i)
    print("Back End performance in MS: %s" % backendPerformance_calc)
    print("Front End performance in MS: %s" % frontendPerformance_calc)
    print("------------------------")

    #-- Writing row wise data in the file
    writer.writerow([backendPerformance_calc,frontendPerformance_calc])
    driver.close()

#---- For closing the CSV file and the WebDriver ----#
driver.quit()
file.close()

#---- To view performance in a dataframe ----#
import pandas as pd
df=pd.read_csv("performance.csv")

#----- Displaying DataFrames output ------#
df